{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "\n",
    "tokenizer =BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# config = BertConfig.from_pretrained('bert-base-chinese')\n",
    "# config.update({'output_hidden_states':True})\n",
    "# model = BertModel.from_pretrained('bert-base-chinese',config = config)\n",
    "\n",
    "from transformers import AutoModel\n",
    "checkpoint = 'bert-base-chinese'\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 5632, 4197, 6427, 6241, 1905, 4415, 102]\n",
      "{'input_ids': [[101, 5381, 5317, 2128, 1059, 2458, 1355, 1146, 711, 676, 702, 2231, 5277, 102], [101, 6756, 6775, 5143, 5320, 2231, 5277, 5381, 5317, 2128, 1059, 2458, 1355, 102], [101, 6756, 6775, 1216, 5543, 2231, 5277, 5381, 5317, 2128, 1059, 2458, 1355, 102], [101, 6756, 6775, 7439, 6956, 816, 2231, 5277, 5381, 5317, 2128, 1059, 2458, 1355, 102], [101, 3844, 6407, 1730, 7339, 3418, 2945, 6756, 6775, 5381, 5317, 2128, 1059, 4680, 3403, 1169, 2137, 3844, 6407, 2825, 3318, 6206, 3724, 1350, 3844, 6407, 6369, 1153, 102], [101, 3844, 6407, 1730, 7339, 1762, 5381, 5317, 2128, 1059, 1730, 7339, 4638, 3118, 2898, 678, 8024, 2130, 2768, 4802, 6371, 3844, 6407, 2400, 5356, 1169, 3844, 6407, 2845, 1440, 102], [101, 1762, 6756, 6775, 4802, 6371, 5310, 3362, 4638, 1825, 4794, 677, 8024, 1825, 754, 1394, 4415, 4638, 4415, 4507, 8024, 4802, 6371, 1762, 6392, 6369, 1469, 2458, 1355, 7348, 3667, 6399, 1166, 1139, 4638, 2792, 3300, 7599, 7372, 1772, 2347, 6158, 2970, 1358, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [101, 671, 6775, 3749, 6756, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('自然语言处理'))\n",
    "sentences = ['网络安全开发分为三个层级',\n",
    "             '车辆系统层级网络安全开发',\n",
    "             '车辆功能层级网络安全开发',\n",
    "             '车辆零部件层级网络安全开发',\n",
    "             '测试团队根据车辆网络安全目标制定测试技术要求及测试计划',\n",
    "             '测试团队在网络安全团队的支持下，完成确认测试并编制测试报告',\n",
    "             '在车辆确认结果的基础上，基于合理的理由，确认在设计和开发阶段识别出的所有风险均已被接受',]\n",
    "test1 = tokenizer(sentences)\n",
    "print(test1)\n",
    "print(tokenizer('一辆汽车'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.28742319345474243,\n",
       "  'token': 6443,\n",
       "  'token_str': '谁',\n",
       "  'sequence': '最 自 由 民 主 的 国 家 是 谁.'},\n",
       " {'score': 0.10349870473146439,\n",
       "  'token': 8038,\n",
       "  'token_str': '：',\n",
       "  'sequence': '最 自 由 民 主 的 国 家 是 ：.'},\n",
       " {'score': 0.06040112301707268,\n",
       "  'token': 8043,\n",
       "  'token_str': '？',\n",
       "  'sequence': '最 自 由 民 主 的 国 家 是 ？.'},\n",
       " {'score': 0.03125236555933952,\n",
       "  'token': 2124,\n",
       "  'token_str': '它',\n",
       "  'sequence': '最 自 由 民 主 的 国 家 是 它.'},\n",
       " {'score': 0.03036455437541008,\n",
       "  'token': 862,\n",
       "  'token_str': '何',\n",
       "  'sequence': '最 自 由 民 主 的 国 家 是 何.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask',model='bert-base-chinese')\n",
    "unmasker('最自由民主的国家是[MASK].',top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "import re\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_path = r\"./imdb_test/aclImdb\"\n",
    "model_path = r'imdb_test/aclImdb/mode'\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self,mode,testNumber = 10000, validNumber = 5000 ) -> None:\n",
    "        super(ImdbDataset,self).__init__()\n",
    "\n",
    "        text_path = [os.path.join(data_base_path,i) for i in ['test/neg', 'test/pos']]\n",
    "        text_path.extend([os.path.join(data_base_path,i) for i in ['train/neg','train/pos']])\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.total_file_path_list = []\n",
    "            for i in text_path:\n",
    "                self.total_file_path_list.extend([os.path.join(i,j) for j in os.listdir(i)])\n",
    "\n",
    "        if mode==\"test\":\n",
    "            self.total_file_path_list = []\n",
    "            # 获取测试数据集，默认10000个数据\n",
    "            for i in text_path:\n",
    "                self.total_file_path_list.extend([os.path.join(i,j) for j in os.listdir(i)])\n",
    "            self.total_file_path_list=sample(self.total_file_path_list,testNumber)\n",
    "       \n",
    "        if mode==\"valid\":\n",
    "            self.total_file_path_list = []\n",
    "            # 获取验证数据集，默认5000个数据集\n",
    "            for i in text_path:\n",
    "                self.total_file_path_list.extend([os.path.join(i,j) for j in os.listdir(i)])\n",
    "            self.total_file_path_list=sample(self.total_file_path_list,validNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./imdb_test/aclImdb/test/neg',\n",
       " './imdb_test/aclImdb/test/pos',\n",
       " './imdb_test/aclImdb/train/neg',\n",
       " './imdb_test/aclImdb/train/pos']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(self,text):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2a7ca6e82e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBertClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertClassificationModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "class BertClassificationModel(nn.Module):\n",
    "    def __init__(self,hidden_size = 768):\n",
    "        super(BertClassificationModel,self).__init__()\n",
    "        model_name = 'distilbert-base-uncased'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "        for p in self.bert.parameters():\n",
    "            p.required_grad = False\n",
    "        self.fc = nn.Linear(hidden_size,2)\n",
    "\n",
    "    def forward(self,batch_sentences):\n",
    "        sentences_tokenizer = self.tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    testNumber = 10000\n",
    "    validNumber = 100\n",
    "    batchsize = 250\n",
    "\n",
    "    trainDatas = ImdbDataset(mode='test',testNumber=testNumber)\n",
    "    validDatas = ImdbDataset(mode='valid',validNumber=validNumber)\n",
    "\n",
    "    train_loader = torch.utils.data.Dataloader(trainDatas,batch_size = batchsize)\n",
    "    val_loader = torch.utils.data.Dataloader(validDatas,batch_size = batchsize)\n",
    "\n",
    "    epoch_num = 1\n",
    "\n",
    "    print('training...')\n",
    "\n",
    "    model = BertClassificationModel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
